{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/akankshamishra/Desktop/DataScience/Data sets/NLP DataSets'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/akankshamishra/Desktop/DataScience/Data sets/NLP DataSets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv(\"new_complaint.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date received</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Sub-issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Company public response</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP code</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Consumer consent provided?</th>\n",
       "      <th>Submitted via</th>\n",
       "      <th>Date sent to company</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>Complaint ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3/12/14</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Other mortgage</td>\n",
       "      <td>Loan modification,collection,foreclosure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M&amp;T BANK CORPORATION</td>\n",
       "      <td>MI</td>\n",
       "      <td>48382</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Referral</td>\n",
       "      <td>3/17/14</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>759217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/19/17</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>Federal student loan servicing</td>\n",
       "      <td>Dealing with my lender or servicer</td>\n",
       "      <td>Received bad information about my loan</td>\n",
       "      <td>When my loan was switched over to Navient i wa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Navient Solutions, LLC.</td>\n",
       "      <td>LA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>1/19/17</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>2296496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4/6/18</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "      <td>General-purpose credit card or charge card</td>\n",
       "      <td>Other features, terms, or problems</td>\n",
       "      <td>Other problem</td>\n",
       "      <td>I tried to sign up for a spending monitoring p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAPITAL ONE FINANCIAL CORPORATION</td>\n",
       "      <td>VA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Older American</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>4/6/18</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2866101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/8/14</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bankruptcy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMERICAN EXPRESS COMPANY</td>\n",
       "      <td>ID</td>\n",
       "      <td>83854</td>\n",
       "      <td>Older American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>6/10/14</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>885638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9/13/14</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>Communication tactics</td>\n",
       "      <td>Frequent or repeated calls</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CITIBANK, N.A.</td>\n",
       "      <td>VA</td>\n",
       "      <td>23233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>9/13/14</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1027760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date received                      Product  \\\n",
       "0       3/12/14                     Mortgage   \n",
       "1       1/19/17                 Student loan   \n",
       "2        4/6/18  Credit card or prepaid card   \n",
       "3        6/8/14                  Credit card   \n",
       "4       9/13/14              Debt collection   \n",
       "\n",
       "                                  Sub-product  \\\n",
       "0                              Other mortgage   \n",
       "1              Federal student loan servicing   \n",
       "2  General-purpose credit card or charge card   \n",
       "3                                         NaN   \n",
       "4                                 Credit card   \n",
       "\n",
       "                                      Issue  \\\n",
       "0  Loan modification,collection,foreclosure   \n",
       "1        Dealing with my lender or servicer   \n",
       "2        Other features, terms, or problems   \n",
       "3                                Bankruptcy   \n",
       "4                     Communication tactics   \n",
       "\n",
       "                                Sub-issue  \\\n",
       "0                                     NaN   \n",
       "1  Received bad information about my loan   \n",
       "2                           Other problem   \n",
       "3                                     NaN   \n",
       "4              Frequent or repeated calls   \n",
       "\n",
       "                        Consumer complaint narrative Company public response  \\\n",
       "0                                                NaN                     NaN   \n",
       "1  When my loan was switched over to Navient i wa...                     NaN   \n",
       "2  I tried to sign up for a spending monitoring p...                     NaN   \n",
       "3                                                NaN                     NaN   \n",
       "4                                                NaN                     NaN   \n",
       "\n",
       "                             Company State ZIP code            Tags  \\\n",
       "0               M&T BANK CORPORATION    MI    48382             NaN   \n",
       "1            Navient Solutions, LLC.    LA      NaN             NaN   \n",
       "2  CAPITAL ONE FINANCIAL CORPORATION    VA      NaN  Older American   \n",
       "3           AMERICAN EXPRESS COMPANY    ID    83854  Older American   \n",
       "4                     CITIBANK, N.A.    VA    23233             NaN   \n",
       "\n",
       "  Consumer consent provided? Submitted via Date sent to company  \\\n",
       "0                        NaN      Referral              3/17/14   \n",
       "1           Consent provided           Web              1/19/17   \n",
       "2           Consent provided           Web               4/6/18   \n",
       "3                        NaN           Web              6/10/14   \n",
       "4                        NaN           Web              9/13/14   \n",
       "\n",
       "  Company response to consumer Timely response? Consumer disputed?  \\\n",
       "0      Closed with explanation              Yes                 No   \n",
       "1      Closed with explanation              Yes                 No   \n",
       "2      Closed with explanation              Yes                NaN   \n",
       "3      Closed with explanation              Yes                Yes   \n",
       "4      Closed with explanation              Yes                Yes   \n",
       "\n",
       "   Complaint ID  \n",
       "0        759217  \n",
       "1       2296496  \n",
       "2       2866101  \n",
       "3        885638  \n",
       "4       1027760  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping the relevant columns\n",
    "data = full_data[[\"Consumer complaint narrative\",\"Product\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "['Mortgage' 'Student loan' 'Credit card or prepaid card' 'Credit card'\n",
      " 'Debt collection' 'Credit reporting'\n",
      " 'Credit reporting, credit repair services, or other personal consumer reports'\n",
      " 'Bank account or service' 'Consumer Loan' 'Money transfers'\n",
      " 'Vehicle loan or lease'\n",
      " 'Money transfer, virtual currency, or money service'\n",
      " 'Checking or savings account' 'Payday loan'\n",
      " 'Payday loan, title loan, or personal loan' 'Other financial service'\n",
      " 'Prepaid card']\n"
     ]
    }
   ],
   "source": [
    "# Printing out the first non-empty value of the X column. Hence the second value, index is 1\n",
    "data.columns = [\"X\",\"y\"]\n",
    "print(data[\"X\"][0])\n",
    "print(data[\"y\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'my', 'loan', 'was', 'switched', 'over', 'to', 'Navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'XXXX', 'i', 'did', 'not.', 'When', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'XXXX', 'into', 'the', 'XXXX.', 'I', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan.', 'I', 'was', 'told', 'that', 'Navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with.', 'I', 'contacted', 'Navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me.', 'I', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus.', 'I', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up.']\n",
      "['When', 'my', 'loan', 'was', 'switched', 'over', 'to', 'Navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'XXXX', 'i', 'did', 'not', '.', 'When', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'XXXX', 'into', 'the', 'XXXX', '.', 'I', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan', '.', 'I', 'was', 'told', 'that', 'Navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with', '.', 'I', 'contacted', 'Navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me', '.', 'I', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus', '.', 'I', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "#Code starts here\n",
    "data.dropna(inplace=True)\n",
    "first_complaint = data[\"X\"].iloc[0]\n",
    "bag_of_words_1 = first_complaint.split()\n",
    "bag_of_words_2 = word_tokenize(first_complaint)\n",
    "print(bag_of_words_1)\n",
    "print(bag_of_words_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_complaint is already loaded onto the workspace\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#Code starts here\n",
    "list_of_sentences = sent_tokenize(first_complaint)\n",
    "first_complaint_lower = first_complaint.lower()\n",
    "bag_of_words_lower = word_tokenize(first_complaint_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not.',\n",
       " 'When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX.',\n",
       " 'I have been faithful at paying my student loan.',\n",
       " 'I was told that Navient was the company i had delinquency with.',\n",
       " 'I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me.',\n",
       " 'I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus.',\n",
       " 'I have had so much trouble bringing my credit score back up.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words of text: Natural Language Processing is really fun and I want to study it more \n",
      "is stemmed in the following way: \n",
      "['natur', 'languag', 'process', 'is', 'realli', 'fun', 'and', 'I', 'want', 'to', 'studi', 'it', 'more']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text=\"Natural Language Processing is really fun and I want to study it more\"\n",
    "print(\"The words of text:\",text,\"\\nis stemmed in the following way: \")\n",
    "\n",
    "#Breaking the sentence to words\n",
    "tokens=text.split()\n",
    "\n",
    "#Defining Porter Stemmer object\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "#Applying the stemming\n",
    "stem = [porter.stem(i) for i in tokens]\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordnet\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/c9/93f89fc3613db301ff92be67aa67a5f9e4b5e212081ce3569e84a9e57304/wordnet-0.0.1b2.tar.gz\n",
      "Collecting colorama==0.3.9 (from wordnet)\n",
      "  Downloading https://files.pythonhosted.org/packages/db/c8/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf/colorama-0.3.9-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: wordnet\n",
      "  Building wheel for wordnet (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wordnet: filename=wordnet-0.0.1b2-cp37-none-any.whl size=10522 sha256=eff1950c5ea9614b6563e609b0da9b1d45c46206c41c4656b66e81bd3a1735d6\n",
      "  Stored in directory: /Users/akankshamishra/Library/Caches/pip/wheels/9f/b7/a9/9f8f3c925c912ac2e8dfa5f8373cd48f18b1074da35b155ad9\n",
      "Successfully built wordnet\n",
      "Installing collected packages: colorama, wordnet\n",
      "  Found existing installation: colorama 0.4.1\n",
      "    Uninstalling colorama-0.4.1:\n",
      "      Successfully uninstalled colorama-0.4.1\n",
      "Successfully installed colorama-0.3.9 wordnet-0.0.1b2\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"Women in  technology are amazing at coding\"\n",
    "#print(\"The words of text:\",text,\"\\nis lemmatized in the following way: \")\n",
    "\n",
    "#tokens=text.lower().split()\n",
    "#lemma =WordNetLemmatizer()\n",
    "#lemma_result = [lemma.lemmatize(i) for i in tokens]\n",
    "#print(lemma_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = Counter(bag_of_words_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'when': 2,\n",
       "         'my': 4,\n",
       "         'loan': 2,\n",
       "         'was': 5,\n",
       "         'switched': 1,\n",
       "         'over': 1,\n",
       "         'to': 5,\n",
       "         'navient': 3,\n",
       "         'i': 11,\n",
       "         'never': 1,\n",
       "         'told': 3,\n",
       "         'that': 3,\n",
       "         'had': 4,\n",
       "         'a': 2,\n",
       "         'deliquint': 1,\n",
       "         'balance': 2,\n",
       "         'because': 1,\n",
       "         'with': 3,\n",
       "         'xxxx': 3,\n",
       "         'did': 1,\n",
       "         'not': 1,\n",
       "         '.': 7,\n",
       "         'going': 1,\n",
       "         'purchase': 1,\n",
       "         'vehicle': 1,\n",
       "         'discovered': 1,\n",
       "         'credit': 4,\n",
       "         'score': 2,\n",
       "         'been': 2,\n",
       "         'dropped': 1,\n",
       "         'from': 1,\n",
       "         'the': 8,\n",
       "         'into': 1,\n",
       "         'have': 2,\n",
       "         'faithful': 1,\n",
       "         'at': 1,\n",
       "         'paying': 1,\n",
       "         'student': 1,\n",
       "         'company': 1,\n",
       "         'delinquency': 2,\n",
       "         'contacted': 1,\n",
       "         'resolve': 1,\n",
       "         'this': 1,\n",
       "         'issue': 1,\n",
       "         'you': 1,\n",
       "         'and': 5,\n",
       "         'kept': 1,\n",
       "         'being': 1,\n",
       "         'just': 2,\n",
       "         'contact': 1,\n",
       "         'bureaus': 2,\n",
       "         'expalin': 1,\n",
       "         'situation': 1,\n",
       "         'maybe': 1,\n",
       "         'they': 1,\n",
       "         'could': 1,\n",
       "         'help': 1,\n",
       "         'me': 1,\n",
       "         'so': 2,\n",
       "         'angry': 1,\n",
       "         'hurried': 1,\n",
       "         'paid': 1,\n",
       "         'off': 1,\n",
       "         'then': 1,\n",
       "         'after': 1,\n",
       "         'tried': 1,\n",
       "         'dispute': 1,\n",
       "         'much': 1,\n",
       "         'trouble': 1,\n",
       "         'bringing': 1,\n",
       "         'back': 1,\n",
       "         'up': 1})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing count vectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the Count Vectorizer object\n",
    "cv = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe \"all text\" with the first 3 rows of data\n",
    "all_text = data[\"X\"][:3]\n",
    "all_text = pd.DataFrame(all_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When my loan was switched over to Navient i wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I tried to sign up for a spending monitoring p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>My mortgage is with BB &amp; T Bank, recently I ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   X\n",
       "1  When my loan was switched over to Navient i wa...\n",
       "2  I tried to sign up for a spending monitoring p...\n",
       "7  My mortgage is with BB & T Bank, recently I ha..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when my loan was switched over to navient i wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i tried to sign up for a spending monitoring p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>my mortgage is with bb &amp; t bank, recently i ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "1  when my loan was switched over to navient i wa...\n",
       "2  i tried to sign up for a spending monitoring p...\n",
       "7  my mortgage is with bb & t bank, recently i ha..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Renaming the column for that dataframe (has only one column) to \"Text\"\n",
    "all_text.columns = [\"Text\"]\n",
    "#Converting to lower case\n",
    "all_text[\"Text\"] = all_text['Text'].str.lower()\n",
    "all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x214 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 251 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the Count vectorizer all text\n",
    "cv.fit(all_text[\"Text\"])\n",
    "#Transforming \"Text\"\n",
    "vector = cv.transform(all_text[\"Text\"])\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 1, 0, 0, 0, 1, 1, 2, 0, 0,\n",
       "        0, 0, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 4, 0, 0, 0, 2, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 4, 2, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 4, 3, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 3, 8,\n",
       "        0, 0, 1, 1, 0, 1, 0, 5, 3, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 5, 0,\n",
       "        0, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 3, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 3, 5, 0, 1, 1, 1, 0, 2, 0, 1, 4,\n",
       "        1, 1, 0, 1, 0, 3, 1, 0, 0, 3, 1, 2, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1,\n",
       "        0, 0, 0, 2, 2, 0, 1, 2, 2, 0, 0, 0, 1, 0, 0, 4, 1, 2, 0, 1, 1, 0,\n",
       "        0, 1, 1, 2, 1, 1, 2, 1, 2, 1, 0, 1, 0, 3, 0, 1, 3, 1, 0, 4, 1, 1,\n",
       "        1, 1, 3, 0, 3, 0, 0, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "        6, 1, 3, 0, 0, 1, 1, 4, 2, 3, 1, 3, 1, 4, 0, 1, 1, 1, 0, 0, 3, 0,\n",
       "        5, 2, 1, 2, 1, 1, 2, 1, 0, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "        2, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 4, 6,\n",
       "        3, 0, 1, 7, 1, 8, 0, 9, 1, 0, 0, 2, 1, 2, 2, 2, 1, 1, 0, 1, 6, 1,\n",
       "        3, 3, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 5, 3]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transforming the vector to array\n",
    "vector_values_array = vector.toarray()\n",
    "vector_values_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 1, 0, 0, 0, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 4, 0, 0, 0, 2, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 4, 2, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 4, 3, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 3, 8, 0, 0, 1, 1, 0, 1, 0, 5, 3, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 3, 0, 0, 1, 0], [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 3, 5, 0, 1, 1, 1, 0, 2, 0, 1, 4, 1, 1, 0, 1, 0, 3, 1, 0, 0, 3, 1, 2, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 0, 0, 0, 2, 2, 0, 1, 2, 2, 0, 0, 0, 1, 0, 0, 4, 1, 2, 0, 1, 1, 0, 0, 1, 1, 2, 1, 1, 2, 1, 2, 1, 0, 1, 0, 3, 0, 1, 3, 1, 0, 4, 1, 1, 1, 1, 3, 0, 3, 0, 0, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 6, 1, 3, 0, 0, 1, 1, 4, 2, 3, 1, 3, 1, 4, 0, 1, 1, 1, 0, 0, 3, 0, 5, 2, 1, 2, 1, 1, 2, 1, 0, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 4, 6, 3, 0, 1, 7, 1, 8, 0, 9, 1, 0, 0, 2, 1, 2, 2, 2, 1, 1, 0, 1, 6, 1, 3, 3, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 5, 3]]\n"
     ]
    }
   ],
   "source": [
    "#Converting the text to numbers - The transform function does this. \n",
    "vector_values_list = vector_values_array.tolist()\n",
    "print(vector_values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rows of vector value list\n",
      " 3\n",
      "\n",
      "The first row of vector_values_list\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 1, 0, 0, 0, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 4, 0, 0, 0, 2, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 4, 2, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 4, 3, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 3, 8, 0, 0, 1, 1, 0, 1, 0, 5, 3, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 3, 0, 0, 1, 0]\n",
      "\n",
      "No of words in first row\n",
      " 214\n"
     ]
    }
   ],
   "source": [
    "#Length of vector value list\n",
    "print (\"No of rows of vector value list\\n\", len(vector_values_list)) \n",
    "print(\"\\nThe first row of vector_values_list\\n\",vector_values_list[0])\n",
    "print (\"\\nNo of words in first row\\n\",len(vector_values_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        labels\n",
      "1                 Student loan\n",
      "2  Credit card or prepaid card\n",
      "7                     Mortgage\n"
     ]
    }
   ],
   "source": [
    "#Creating a dataframe for target\n",
    "labels = pd.DataFrame(data[\"y\"][:3])\n",
    "labels.columns = [\"labels\"]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   labels\n",
      "1       2\n",
      "2       0\n",
      "7       1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "labels[\"labels\"] = le.fit_transform(labels[\"labels\"])\n",
    "print (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When my loan was switched over to Navient i wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I tried to sign up for a spending monitoring p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>My mortgage is with BB &amp; T Bank, recently I ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The entire lending experience with Citizens Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>My credit score has gone down XXXX points in t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    X\n",
       "1   When my loan was switched over to Navient i wa...\n",
       "2   I tried to sign up for a spending monitoring p...\n",
       "7   My mortgage is with BB & T Bank, recently I ha...\n",
       "13  The entire lending experience with Citizens Ba...\n",
       "14  My credit score has gone down XXXX points in t..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text = data[[\"X\"]]\n",
    "all_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_text[\"X\"] = all_text['X'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "vector = cv.fit_transform(all_text[\"X\"])\n",
    "X = vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "labels = data[[\"y\"]]\n",
    "le = LabelEncoder()\n",
    "labels[\"y\"] = le.fit_transform(labels[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4925373134328358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test  = train_test_split(X,labels[\"y\"], random_state = 42, test_size = 0.4)\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train,y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "acc = accuracy_score(y_pred,y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/akankshamishra/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ve', 'through', \"isn't\", 'your', 'had', 'needn', 'as', 'very', 'whom', 'i', 'has', 'out', 'under', 'a', 'an', 'at', 'o', 'few', 'into', 'don', 'herself', 'do', 'those', 'doesn', 'his', 'or', 'this', \"didn't\", \"wouldn't\", 'while', 'of', 'should', \"you've\", \"haven't\", 'here', \"mustn't\", 'up', 'because', 'haven', 'again', 'and', 'there', 'mustn', 'on', \"aren't\", 'ours', 'any', 'is', 'ourselves', \"don't\", 's', 'll', \"weren't\", 'are', \"you're\", 'shouldn', 'in', 'not', 'my', 'after', 'we', 'about', 'yourselves', 'but', 'than', 'didn', 'wasn', 'been', 'other', 'both', 'our', 'if', 'what', \"hasn't\", 'having', 'd', 'was', 'such', 'who', \"it's\", 'from', 'against', 'more', \"doesn't\", 'isn', 'her', \"needn't\", 'their', 'them', 'he', 'same', 'did', 'only', 'when', 'some', 'most', 'all', 'they', 'himself', 'theirs', 'myself', 'no', 'him', 'y', \"won't\", 'being', 'once', 'that', \"hadn't\", 'won', 'down', \"that'll\", 'these', 'where', 'doing', 'hadn', 'each', 'itself', 'off', 'before', 'above', 'yourself', \"shouldn't\", 'with', 'which', 'were', 'ma', 'just', 'hers', 'now', 'over', 'so', 'until', 'm', 'am', 'shan', 'be', 'own', 'ain', 'yours', \"shan't\", 'does', 'why', 'between', 'the', 're', 'weren', 'themselves', 'below', \"couldn't\", 'she', 'wouldn', 'nor', 'couldn', 't', \"mightn't\", 'during', 'further', \"wasn't\", 'how', 'will', 'can', 'hasn', 'to', 'it', \"should've\", 'then', 'mightn', 'you', \"you'll\", 'aren', \"she's\", 'have', 'too', \"you'd\", 'by', 'me', 'for', 'its'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "print (set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "print (list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_set_of_stopwords = set(stopwords.words('english')+list(punctuation)+[\"medicine\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Complaint:\n",
      " When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.\n",
      "\n",
      "Bag of words of first complaint:\n",
      " ['When', 'my', 'loan', 'was', 'switched', 'over', 'to', 'Navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'XXXX', 'i', 'did', 'not', '.', 'When', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'XXXX', 'into', 'the', 'XXXX', '.', 'I', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan', '.', 'I', 'was', 'told', 'that', 'Navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with', '.', 'I', 'contacted', 'Navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me', '.', 'I', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus', '.', 'I', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up', '.']\n",
      "\n",
      "Len of bag of words:\n",
      " 137\n",
      "\n",
      "Bag of words with stopwords removed:\n",
      " ['When', 'loan', 'switched', 'Navient', 'never', 'told', 'deliquint', 'balance', 'XXXX', 'When', 'going', 'purchase', 'vehicle', 'discovered', 'credit', 'score', 'dropped', 'XXXX', 'XXXX', 'I', 'faithful', 'paying', 'student', 'loan', 'I', 'told', 'Navient', 'company', 'delinquency', 'I', 'contacted', 'Navient', 'resolve', 'issue', 'kept', 'told', 'contact', 'credit', 'bureaus', 'expalin', 'situation', 'maybe', 'could', 'help', 'I', 'angry', 'hurried', 'paid', 'balance', 'tried', 'dispute', 'delinquency', 'credit', 'bureaus', 'I', 'much', 'trouble', 'bringing', 'credit', 'score', 'back']\n",
      "Len of bag of words with stopwords removed:\n",
      " 61\n"
     ]
    }
   ],
   "source": [
    "#Storing the first complaint\n",
    "first_complaint = data.iloc[0][0]\n",
    "\n",
    "print(\"\\nFirst Complaint:\\n\",first_complaint)\n",
    "\n",
    "bag_of_words = word_tokenize(first_complaint)\n",
    "\n",
    "print (\"\\nBag of words of first complaint:\\n\",bag_of_words)\n",
    "print(\"\\nLen of bag of words:\\n\",len(bag_of_words))\n",
    "\n",
    "#Removing stopwords\n",
    "bow_stopwords_removed = [x for x in bag_of_words if x not in custom_set_of_stopwords]\n",
    "\n",
    "print (\"\\nBag of words with stopwords removed:\\n\",bow_stopwords_removed)\n",
    "\n",
    "print(\"Len of bag of words with stopwords removed:\\n\",len(bow_stopwords_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5597014925373134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cv_stop = CountVectorizer(stop_words = \"english\")\n",
    "vector_stop = cv_stop.fit_transform(data[\"X\"])\n",
    "X_stop = vector_stop.toarray()\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_stop,labels[\"y\"], random_state = 42, test_size = 0.4)\n",
    "\n",
    "log_reg = LogisticRegression(random_state = 42)\n",
    "log_reg.fit(X_train,y_train)\n",
    "stop_acc = log_reg.score(X_test,y_test)\n",
    "print(stop_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complaint 1:  When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.\n",
      "\n",
      "Complaint 2:  I tried to sign up for a spending monitoring program and Capital One will not let me access my account through them\n",
      "\n",
      "Complaint 3:  My mortgage is with BB & T Bank, recently I have been investigating ways to pay down my mortgage faster and I came across Biweekly Mortgage Calculator on BB & T 's website. It's a nice, easy to use calculator that you plug in your interest rate, mortgage amount, mortgage term, and payment type and it calculates your accelerated bi-weekly payment for you and shows you how much quicker you can pay down your loan. Ours figured out to pay off a 30 year mortgage in 26.4 years ... quite a savings! \n",
      "I called BB & T 's customer service number to inquire how I get set up on this payment plan. I was told they do not offer that type of payment plan, but I could send in my payments bi-weekly but it would not be applied until the full amount was received. ( the money would sit in a \" holding account '' until the full payment amount was collected ). I ended up calling back a few days later thinking the rep I was talking to didn't understand what I wanted to do or was not knowledgeable of this program. I got the SAME ANSWER! \n",
      "I then asked for the corporate BB & T office number where I could speak to someone that was knowledgeable of this product. After 3 days I received a phone call back from a corporate manager stating they do not offer this product, and they were \" checking into why this is on their website ''. She stated they do have a few customers that make bi-weekly payments, but they no longer offer this service. \n",
      "I don't understand how they can have this active link on their website under their Financial Planning Center tab to mislead customers when all they say is \" I'm sorry, I know you're upset about this '' Sounds like false advertising to me! \n",
      "https : //www.bbt.com/XXXX\n"
     ]
    }
   ],
   "source": [
    "complaint_1 = data[\"X\"].iloc[0]\n",
    "complaint_2 = data[\"X\"].iloc[1]\n",
    "complaint_3 = data[\"X\"].iloc[2]\n",
    "print (\"Complaint 1: \", complaint_1)\n",
    "print (\"\\nComplaint 2: \", complaint_2)\n",
    "print (\"\\nComplaint 3: \", complaint_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the vectorized sentence: (3, 214)\n",
      "The tf-idf score of first five elements: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Dictionary of words with tf-idf score:\n",
      " {'26': 0.0, '30': 0.0, 'about': 0.0, 'accelerated': 0.0, 'access': 0.0, 'account': 0.0, 'across': 0.0, 'active': 0.0, 'advertising': 0.0, 'after': 0.05253580411952334, 'all': 0.0, 'amount': 0.0, 'and': 0.20399369240069398, 'angry': 0.06907826902804956, 'answer': 0.0, 'applied': 0.0, 'asked': 0.0, 'at': 0.06907826902804956, 'back': 0.05253580411952334, 'balance': 0.13815653805609912, 'bank': 0.0, 'bb': 0.0, 'bbt': 0.0, 'be': 0.0, 'because': 0.06907826902804956, 'been': 0.10507160823904668, 'being': 0.06907826902804956, 'bi': 0.0, 'biweekly': 0.0, 'bringing': 0.06907826902804956, 'bureaus': 0.13815653805609912, 'but': 0.0, 'calculates': 0.0, 'calculator': 0.0, 'call': 0.0, 'called': 0.0, 'calling': 0.0, 'came': 0.0, 'can': 0.0, 'capital': 0.0, 'center': 0.0, 'checking': 0.0, 'collected': 0.0, 'com': 0.0, 'company': 0.06907826902804956, 'contact': 0.06907826902804956, 'contacted': 0.06907826902804956, 'corporate': 0.0, 'could': 0.05253580411952334, 'credit': 0.27631307611219824, 'customer': 0.0, 'customers': 0.0, 'days': 0.0, 'delinquency': 0.13815653805609912, 'deliquint': 0.06907826902804956, 'did': 0.06907826902804956, 'didn': 0.0, 'discovered': 0.06907826902804956, 'dispute': 0.06907826902804956, 'do': 0.0, 'don': 0.0, 'down': 0.0, 'dropped': 0.06907826902804956, 'easy': 0.0, 'ended': 0.0, 'expalin': 0.06907826902804956, 'faithful': 0.06907826902804956, 'false': 0.0, 'faster': 0.0, 'few': 0.0, 'figured': 0.0, 'financial': 0.0, 'for': 0.0, 'from': 0.05253580411952334, 'full': 0.0, 'get': 0.0, 'going': 0.06907826902804956, 'got': 0.0, 'had': 0.27631307611219824, 'have': 0.10507160823904668, 'help': 0.06907826902804956, 'holding': 0.0, 'how': 0.0, 'https': 0.0, 'hurried': 0.06907826902804956, 'in': 0.0, 'inquire': 0.0, 'interest': 0.0, 'into': 0.05253580411952334, 'investigating': 0.0, 'is': 0.0, 'issue': 0.06907826902804956, 'it': 0.0, 'just': 0.13815653805609912, 'kept': 0.06907826902804956, 'know': 0.0, 'knowledgeable': 0.0, 'later': 0.0, 'let': 0.0, 'like': 0.0, 'link': 0.0, 'loan': 0.10507160823904668, 'longer': 0.0, 'make': 0.0, 'manager': 0.0, 'maybe': 0.06907826902804956, 'me': 0.0407987384801388, 'mislead': 0.0, 'money': 0.0, 'monitoring': 0.0, 'mortgage': 0.0, 'much': 0.05253580411952334, 'my': 0.1631949539205552, 'navient': 0.20723480708414868, 'never': 0.06907826902804956, 'nice': 0.0, 'no': 0.0, 'not': 0.0407987384801388, 'number': 0.0, 'of': 0.0, 'off': 0.05253580411952334, 'offer': 0.0, 'office': 0.0, 'on': 0.0, 'one': 0.0, 'or': 0.0, 'ours': 0.0, 'out': 0.0, 'over': 0.06907826902804956, 'paid': 0.06907826902804956, 'pay': 0.0, 'paying': 0.06907826902804956, 'payment': 0.0, 'payments': 0.0, 'phone': 0.0, 'plan': 0.0, 'planning': 0.0, 'plug': 0.0, 'product': 0.0, 'program': 0.0, 'purchase': 0.06907826902804956, 'quicker': 0.0, 'quite': 0.0, 'rate': 0.0, 're': 0.0, 'received': 0.0, 'recently': 0.0, 'rep': 0.0, 'resolve': 0.06907826902804956, 'same': 0.0, 'savings': 0.0, 'say': 0.0, 'score': 0.13815653805609912, 'send': 0.0, 'service': 0.0, 'set': 0.0, 'she': 0.0, 'shows': 0.0, 'sign': 0.0, 'sit': 0.0, 'situation': 0.06907826902804956, 'so': 0.13815653805609912, 'someone': 0.0, 'sorry': 0.0, 'sounds': 0.0, 'speak': 0.0, 'spending': 0.0, 'stated': 0.0, 'stating': 0.0, 'student': 0.06907826902804956, 'switched': 0.06907826902804956, 'tab': 0.0, 'talking': 0.0, 'term': 0.0, 'that': 0.15760741235857004, 'the': 0.42028643295618673, 'their': 0.0, 'them': 0.0, 'then': 0.05253580411952334, 'they': 0.05253580411952334, 'thinking': 0.0, 'this': 0.05253580411952334, 'through': 0.0, 'to': 0.20399369240069398, 'told': 0.15760741235857004, 'tried': 0.05253580411952334, 'trouble': 0.06907826902804956, 'type': 0.0, 'under': 0.0, 'understand': 0.0, 'until': 0.0, 'up': 0.0407987384801388, 'upset': 0.0, 'use': 0.0, 'vehicle': 0.06907826902804956, 'wanted': 0.0, 'was': 0.2626790205976167, 'ways': 0.0, 'website': 0.0, 'weekly': 0.0, 'were': 0.0, 'what': 0.0, 'when': 0.10507160823904668, 'where': 0.0, 'why': 0.0, 'will': 0.0, 'with': 0.15760741235857004, 'would': 0.0, 'www': 0.0, 'xxxx': 0.15760741235857004, 'year': 0.0, 'years': 0.0, 'you': 0.05253580411952334, 'your': 0.0}\n",
      "Sorted dictonary:\n",
      "\n",
      "[('the', 0.42028643295618673), ('credit', 0.27631307611219824), ('had', 0.27631307611219824), ('was', 0.2626790205976167), ('navient', 0.20723480708414868), ('and', 0.20399369240069398), ('to', 0.20399369240069398), ('my', 0.1631949539205552), ('that', 0.15760741235857004), ('told', 0.15760741235857004), ('with', 0.15760741235857004), ('xxxx', 0.15760741235857004), ('balance', 0.13815653805609912), ('bureaus', 0.13815653805609912), ('delinquency', 0.13815653805609912), ('just', 0.13815653805609912), ('score', 0.13815653805609912), ('so', 0.13815653805609912), ('been', 0.10507160823904668), ('have', 0.10507160823904668), ('loan', 0.10507160823904668), ('when', 0.10507160823904668), ('angry', 0.06907826902804956), ('at', 0.06907826902804956), ('because', 0.06907826902804956), ('being', 0.06907826902804956), ('bringing', 0.06907826902804956), ('company', 0.06907826902804956), ('contact', 0.06907826902804956), ('contacted', 0.06907826902804956), ('deliquint', 0.06907826902804956), ('did', 0.06907826902804956), ('discovered', 0.06907826902804956), ('dispute', 0.06907826902804956), ('dropped', 0.06907826902804956), ('expalin', 0.06907826902804956), ('faithful', 0.06907826902804956), ('going', 0.06907826902804956), ('help', 0.06907826902804956), ('hurried', 0.06907826902804956), ('issue', 0.06907826902804956), ('kept', 0.06907826902804956), ('maybe', 0.06907826902804956), ('never', 0.06907826902804956), ('over', 0.06907826902804956), ('paid', 0.06907826902804956), ('paying', 0.06907826902804956), ('purchase', 0.06907826902804956), ('resolve', 0.06907826902804956), ('situation', 0.06907826902804956), ('student', 0.06907826902804956), ('switched', 0.06907826902804956), ('trouble', 0.06907826902804956), ('vehicle', 0.06907826902804956), ('after', 0.05253580411952334), ('back', 0.05253580411952334), ('could', 0.05253580411952334), ('from', 0.05253580411952334), ('into', 0.05253580411952334), ('much', 0.05253580411952334), ('off', 0.05253580411952334), ('then', 0.05253580411952334), ('they', 0.05253580411952334), ('this', 0.05253580411952334), ('tried', 0.05253580411952334), ('you', 0.05253580411952334), ('me', 0.0407987384801388), ('not', 0.0407987384801388), ('up', 0.0407987384801388), ('26', 0.0), ('30', 0.0), ('about', 0.0), ('accelerated', 0.0), ('access', 0.0), ('account', 0.0), ('across', 0.0), ('active', 0.0), ('advertising', 0.0), ('all', 0.0), ('amount', 0.0), ('answer', 0.0), ('applied', 0.0), ('asked', 0.0), ('bank', 0.0), ('bb', 0.0), ('bbt', 0.0), ('be', 0.0), ('bi', 0.0), ('biweekly', 0.0), ('but', 0.0), ('calculates', 0.0), ('calculator', 0.0), ('call', 0.0), ('called', 0.0), ('calling', 0.0), ('came', 0.0), ('can', 0.0), ('capital', 0.0), ('center', 0.0), ('checking', 0.0), ('collected', 0.0), ('com', 0.0), ('corporate', 0.0), ('customer', 0.0), ('customers', 0.0), ('days', 0.0), ('didn', 0.0), ('do', 0.0), ('don', 0.0), ('down', 0.0), ('easy', 0.0), ('ended', 0.0), ('false', 0.0), ('faster', 0.0), ('few', 0.0), ('figured', 0.0), ('financial', 0.0), ('for', 0.0), ('full', 0.0), ('get', 0.0), ('got', 0.0), ('holding', 0.0), ('how', 0.0), ('https', 0.0), ('in', 0.0), ('inquire', 0.0), ('interest', 0.0), ('investigating', 0.0), ('is', 0.0), ('it', 0.0), ('know', 0.0), ('knowledgeable', 0.0), ('later', 0.0), ('let', 0.0), ('like', 0.0), ('link', 0.0), ('longer', 0.0), ('make', 0.0), ('manager', 0.0), ('mislead', 0.0), ('money', 0.0), ('monitoring', 0.0), ('mortgage', 0.0), ('nice', 0.0), ('no', 0.0), ('number', 0.0), ('of', 0.0), ('offer', 0.0), ('office', 0.0), ('on', 0.0), ('one', 0.0), ('or', 0.0), ('ours', 0.0), ('out', 0.0), ('pay', 0.0), ('payment', 0.0), ('payments', 0.0), ('phone', 0.0), ('plan', 0.0), ('planning', 0.0), ('plug', 0.0), ('product', 0.0), ('program', 0.0), ('quicker', 0.0), ('quite', 0.0), ('rate', 0.0), ('re', 0.0), ('received', 0.0), ('recently', 0.0), ('rep', 0.0), ('same', 0.0), ('savings', 0.0), ('say', 0.0), ('send', 0.0), ('service', 0.0), ('set', 0.0), ('she', 0.0), ('shows', 0.0), ('sign', 0.0), ('sit', 0.0), ('someone', 0.0), ('sorry', 0.0), ('sounds', 0.0), ('speak', 0.0), ('spending', 0.0), ('stated', 0.0), ('stating', 0.0), ('tab', 0.0), ('talking', 0.0), ('term', 0.0), ('their', 0.0), ('them', 0.0), ('thinking', 0.0), ('through', 0.0), ('type', 0.0), ('under', 0.0), ('understand', 0.0), ('until', 0.0), ('upset', 0.0), ('use', 0.0), ('wanted', 0.0), ('ways', 0.0), ('website', 0.0), ('weekly', 0.0), ('were', 0.0), ('what', 0.0), ('where', 0.0), ('why', 0.0), ('will', 0.0), ('would', 0.0), ('www', 0.0), ('year', 0.0), ('years', 0.0), ('your', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents called sents\n",
    "sents = [complaint_1, complaint_2, complaint_3]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(sents)\n",
    "vector = vectorizer.transform(sents)\n",
    "print(\"Shape of the vectorized sentence:\",vector.shape)\n",
    "vector_values = vector.toarray().tolist()[0]\n",
    "print(\"The tf-idf score of first five elements:\",vector_values[:5])\n",
    "# Converting the tf-idf score with the word into a dictionary\n",
    "import operator\n",
    "sorted_x = sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "words = [x[0] for x in sorted_x]\n",
    "d = dict(zip(words,vector_values))\n",
    "print(\"Dictionary of words with tf-idf score:\\n\", d)\n",
    "#Sorting this dictionary by value in the descending order to see the ranking\n",
    "print(\"Sorted dictonary:\\n\")\n",
    "print (sorted(d.items(), key=operator.itemgetter(1), reverse = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted dictionary: \n",
      "\n",
      "[('navient', 0.35431369455512246), ('the', 0.24041090745878946), ('delinquency', 0.23620912970341496), ('had', 0.20963581996093744), ('told', 0.19801036084460227), ('bureaus', 0.19189624902422267), ('was', 0.18441012909485335), ('score', 0.1637072393027124), ('credit', 0.15907030235158917), ('just', 0.15203704157649714), ('balance', 0.14549112699503114), ('to', 0.14437952896171652), ('and', 0.14013869297167467), ('loan', 0.1344398602659683), ('angry', 0.12870728663065903), ('deliquint', 0.12870728663065903), ('expalin', 0.12870728663065903), ('faithful', 0.12870728663065903), ('hurried', 0.12870728663065903), ('switched', 0.12870728663065903)]\n"
     ]
    }
   ],
   "source": [
    "sents=[]\n",
    "for x in range(100):\n",
    "   sents.append(data[\"X\"].iloc[x])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents called sents\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(sents)\n",
    "vector = vectorizer.transform(sents)\n",
    "vector.shape \n",
    "vector_values = vector.toarray().tolist()[0]\n",
    "sorted_x = sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "words = [x[0] for x in sorted_x]\n",
    "d = dict(zip(words,vector_values))\n",
    "print(\"Sorted dictionary: \\n\")\n",
    "print ((sorted(d.items(), key=operator.itemgetter(1), reverse = True))[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43283582089552236"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Code starts here\n",
    "tfidf = TfidfVectorizer(stop_words = \"english\")\n",
    "vector_tfidf = tfidf.fit_transform(data[\"X\"])\n",
    "X_tfidf = vector_tfidf.toarray()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_tfidf,labels[\"y\"],random_state = 42, test_size = 0.4)\n",
    "log_reg = LogisticRegression(random_state = 42)\n",
    "log_reg.fit(X_train,y_train)\n",
    "tfidf_acc = log_reg.score(X_test,y_test)\n",
    "tfidf_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
